# -*- coding: utf-8 -*-
"""mldl-project1-emotion-cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6r1u8G0QzsH2hPVe4lpnfc99BB4s9sI
"""

# ! pip install zipfile
import os
import shutil
import random
from zipfile import ZipFile

# data preprocessing

# desired output: x_train, x_valid, y_train, y_valid
base_data_path = "./emotion_data/images"
train_data_path = base_data_path + "/train"
valid_data_path = base_data_path + "/validation"

# renaming train images
for class_name in os.listdir(train_data_path):
    for old_image_name in os.listdir(f"{train_data_path}/{class_name}"):
        old_image_path = f"{train_data_path}/{class_name}/{old_image_name}"
        image_number = old_image_name[:-4]
        new_image_path = (
            f"{train_data_path}/{class_name}/" + f"{class_name}_{image_number}.jpg"
        )
        os.rename(old_image_path, new_image_path)
    print("renamed class", class_name)

# renaming valid images
for class_name in os.listdir(valid_data_path):
    for old_image_name in os.listdir(f"{valid_data_path}/{class_name}"):
        old_image_path = f"{valid_data_path}/{class_name}/{old_image_name}"
        image_number = old_image_name[:-4]
        new_image_path = (
            f"{valid_data_path}/{class_name}/" + f"{class_name}_{image_number}.jpg"
        )
        os.rename(old_image_path, new_image_path)
    print("renamed class", class_name)

# load training images into numpy array with labels

train_image_names_list = []
for class_name in os.listdir(train_data_path):
    class_images_list = os.listdir(f"{train_data_path}/{class_name}")
    train_image_names_list += class_images_list

print(len(train_image_names_list))

# load validation images into numpy array with labels
valid_image_names_list = []
for class_name in os.listdir(valid_data_path):
    class_images_list = os.listdir(f"{valid_data_path}/{class_name}")
    valid_image_names_list += class_images_list

print(len(valid_image_names_list))
print(valid_image_names_list)

name = "disgust_12232.jpg"
index = name.find("_")
name[:index]

# demo with PIL library
# import PIL
# from PIL import Image
# import IPython.display as display
# import numpy as np
#
# image = Image.open("/content/9541.jpg")
# image_tensor = np.asarray(image)
# image_tensor = image_tensor / 255
# image_tensor = image_tensor.reshape(48, 48, 1)
# display.display(image)
# # image_tensor.shape

# image2 = Image.open("/content/3.jpg").convert('L')

# display.display(image2)

# RGB TRAIN IMAGES: creating numpy list of tuples of (image tensor, label)
import cv2
import numpy as np

# from google.colab.patches import cv2_imshow
from PIL import Image
import IPython.display as display

base_train_image_path = "./emotion_data/train"

train_data_list = []

count = 0
for image_name in train_image_names_list:
    underscore_index = image_name.find("_")
    class_name = image_name[:underscore_index]
    PATH = f"{base_train_image_path}/{class_name}/{image_name}"
    image = Image.open(PATH).convert("L")

    image_tensor = np.asarray(image)
    image_tensor = image_tensor / 255
    image_tensor = image_tensor.reshape(48, 48, 1)  # reshaping for keras

    curr_tuple = (image_tensor, class_name)
    train_data_list.append(curr_tuple)
    if count % 100 == 0:
        print("at image", count)
    count += 1

train_data = np.array(train_data_list, dtype=object)

# Training the data

# load validating images into numpy array with labels

valid_image_names_list = []
for class_name in os.listdir(valid_data_path):
    class_images_list = os.listdir(f"{valid_data_path}/{class_name}")
    valid_image_names_list += class_images_list

print(len(valid_image_names_list))

# RGB Validation Image: creating numpy list of tuples of (image tensor, lable)
import cv2
import numpy as np
from google.colab.patches import cv2_imshow

base_validation_path = "./emotion_data/validation"

validation_data_list = []
for image_name in validation_image_names_list:
    underscore_index = image_name.find("_")
    class_name = image_name[:underscore_index]
    PATH = f"{base_validation_image_path}/{class_name}/{image_name}"
    image = cv2.imread(PATH, flags=0)
    # cv2_imshow(image)

    image_tensor = np.array(image)
    image_tensor = image_tensor / 255
    image_tensor = image_tensor.reshape(48, 48, 1)  # reshaping for keras

    curr_tuple = (image_tensor, class_name)
    validation_data_list.append(curr_tuple)

validation_data = np.array(validation_data_list, dtype=object)

# Using image_dataset_from_directory
import os
import PIL
import PIL.Image
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds

import pathlib

train_dir = "./emotion_data/train"
val_dir = "./emotion_data/validation"
seed = 1
image_height = 48
image_width = 48
batch_size = 32
num_classes = 7

train_dataset = tf.keras.utils.image_dataset_from_directory(
    directory=train_dir,
    labels="inferred",
    label_mode="categorical",
    class_names=None,
    color_mode="grayscale",
    shuffle=True,
    seed=seed,
    image_size=(image_height, image_width),
    batch_size=batch_size,
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    directory=val_dir,
    labels="inferred",
    label_mode="categorical",
    shuffle=True,
    seed=seed,
    image_size=(image_height, image_width),
    batch_size=batch_size,
)

class_names = train_dataset.class_names
class_names

AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)
val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)

from tensorflow.keras.layers import Rescaling, Conv2D, MaxPooling2D, Flatten, Dense

num_classes = 7

model = tf.keras.Sequential(
    [
        Rescaling(1.0 / 255),
        Conv2D(filters=16, kernel_size=3, strides=(2, 2), activation="relu"),
        MaxPooling2D(),
        # Conv2D(filters=32, kernel_size=3, strides=(2,2), activation='relu'),
        # MaxPooling2D(),
        # Conv2D(filters=32, kernel_size=3, activation='relu'),
        # MaxPooling2D(),
        Flatten(),
        # Dense(64, activation='relu'),
        Dense(num_classes),
    ]
)

model.compile(
    optimizer="adam",
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)

model.fit(x=train_dataset, validation_data=val_dataset, epochs=10)
#
# # from google.colab import drive
# #
# # drive.mount("/content/drive")
#
# # keras model
# num_filters = 8
# filter_size1 = 3
# pool_size1 = 2
#
# cnn_model.add(
#     Conv2D(num_filters, filter_size1, strides=(1, 1), input_shape=(48, 48, 1))
# )
# cnn_model.add(MaxPooling2D(pool_size=pool_size1))
#
# cnn_model.add(
#     Conv2D(num_filters, filter_size1, strides=(1, 1), input_shape=(45, 45, num_filters))
# )
# cnn_model.add(MaxPooling2D(pool_size=pool_size1))
#
# cnn_model.add(Flatten())
# cnn_model.add(Dense(10, activation="softmax"))
#
# # compile
# cnn_model.compile("adam", loss="categorical_crossentropy", metrics=["accuracy"])
# # train
# model.fit(
#     x_train,
#     to_categorical(y_train),
#     epochs=3,
#     verbose=1,
#     validation_data=(x_test, to_categorical(y_test)),
# )
